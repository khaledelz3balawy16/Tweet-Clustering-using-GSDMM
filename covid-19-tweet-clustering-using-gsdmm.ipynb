{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10218308,"sourceType":"datasetVersion","datasetId":6316465}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# COVID-19 Tweet Clustering using GSDMM\n\nThis notebook performs clustering of COVID-19-related tweets using the **GSDMM (Gibbs Sampling for Dirichlet Multinomial Mixture Model)** algorithm. The main steps involve text preprocessing, tokenization, lemmatization, and clustering of tweets into groups based on similar content. Below is a summary of the approach and methodology used:\n\n### Methodology:\n1. **Data Collection**: \n   - The dataset contains tweets related to COVID-19, with a focus on sentiment analysis.\n   \n2. **Text Preprocessing**: \n   - Tokenization: The tweets are split into individual words.\n   - Removal of stopwords: Common words (e.g., \"the\", \"and\", etc.) are removed to focus on meaningful words.\n   - Lemmatization: Words are reduced to their base form (e.g., \"running\" to \"run\").\n   - N-grams Creation: Bigram and trigram models are generated to capture word relationships.\n\n3. **Modeling**: \n   - GSDMM is applied to the preprocessed tweets, where different combinations of hyperparameters (Alpha and Beta) are tested to optimize clustering.\n   - The optimal number of clusters (K) is set to 4 for this analysis.\n\n4. **Clustering and Evaluation**:\n   - Each tweet is assigned to the most probable cluster based on the word distribution learned by the model.\n   - The results are evaluated based on the loss function, which measures how well the documents are distributed across clusters.\n\n5. **Results**: \n   - The final output includes the cluster assignments for each tweet, along with the most frequent words in each cluster.\n   - Results are saved into a CSV file named `ClusteredTweets.csv`.\n\n### Key Functions:\n- `sent_to_words()`: Tokenizes the tweet texts into words.\n- `make_n_grams()`: Generates bigrams and trigrams to enhance the model's understanding of word relationships.\n- `remove_stopwords()`: Removes common stopwords to focus on meaningful words.\n- `lemmatization()`: Reduces words to their base form using the Spacy NLP library.\n- `choose_best_label()`: Assigns each tweet to the most likely cluster based on the word distribution in each cluster.\n\n### Requirements:\n- **Python 3.x**\n- **Libraries**:\n  - `numpy`\n  - `pandas`\n  - `gsdmm`\n  - `gensim`\n  - `spacy`\n  - `nltk`\n\n### How to Run:\n1. Install the required libraries:\n   ```bash\n   pip install numpy pandas gsdmm gensim spacy nltk\n   ```\n2. Download the Spacy English model:\n   ```bash\n   python -m spacy download en_core_web_sm\n   ```\n3. Run the notebook and observe the results. The final output CSV file will be saved as `ClusteredTweets.csv`.\n\n","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:41.950251Z","iopub.execute_input":"2024-12-23T04:46:41.950603Z","iopub.status.idle":"2024-12-23T04:46:41.959067Z","shell.execute_reply.started":"2024-12-23T04:46:41.950572Z","shell.execute_reply":"2024-12-23T04:46:41.958236Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/covid-19-tweets-for-sentiment-analysis/Corona.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"pip install git+https://github.com/rwalk/gsdmm.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:41.960501Z","iopub.execute_input":"2024-12-23T04:46:41.960867Z","iopub.status.idle":"2024-12-23T04:46:51.229380Z","shell.execute_reply.started":"2024-12-23T04:46:41.960820Z","shell.execute_reply":"2024-12-23T04:46:51.228207Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/rwalk/gsdmm.git\n  Cloning https://github.com/rwalk/gsdmm.git to /tmp/pip-req-build-g__op5s_\n  Running command git clone --filter=blob:none --quiet https://github.com/rwalk/gsdmm.git /tmp/pip-req-build-g__op5s_\n  Resolved https://github.com/rwalk/gsdmm.git to commit 4ad1b6b6976743681ee4976b4573463d359214ee\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from gsdmm==0.1) (1.26.4)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"### استيراد المكتبات المطلوبة مثل numpy و pandas و gsdmm و gensim و spacy\n### Importing the required libraries like numpy, pandas, gsdmm, gensim, and spacy","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport pandas as pd\nfrom gsdmm import MovieGroupProcess\nfrom gensim.utils import simple_preprocess\nimport gensim, spacy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:51.231794Z","iopub.execute_input":"2024-12-23T04:46:51.232541Z","iopub.status.idle":"2024-12-23T04:46:51.236899Z","shell.execute_reply.started":"2024-12-23T04:46:51.232505Z","shell.execute_reply":"2024-12-23T04:46:51.236090Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### تحميل نموذج اللغة الإنجليزية من Spacy لتسهيل عمليات المعالجة اللغوية مثل الجذرة (lemmatization)\n### Loading the English language model from Spacy for easier NLP tasks like lemmatization","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:51.238033Z","iopub.execute_input":"2024-12-23T04:46:51.238300Z","iopub.status.idle":"2024-12-23T04:46:51.862549Z","shell.execute_reply.started":"2024-12-23T04:46:51.238273Z","shell.execute_reply":"2024-12-23T04:46:51.861834Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# قراءة البيانات من ملف CSV يحتوي على تغريدات حول فيروس كورونا\n# Reading the data from a CSV file that contains tweets about COVID-19\ndata = pd.read_csv(r'/kaggle/input/covid-19-tweets-for-sentiment-analysis/Corona.csv', header=0, encoding='cp437')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:51.864791Z","iopub.execute_input":"2024-12-23T04:46:51.865458Z","iopub.status.idle":"2024-12-23T04:46:52.124952Z","shell.execute_reply.started":"2024-12-23T04:46:51.865414Z","shell.execute_reply":"2024-12-23T04:46:52.124227Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"### تحويل النصوص إلى كلمات باستخدام دالة simple_preprocess من مكتبة gensim\n### Converting tweets to words using the simple_preprocess function from gensim","metadata":{}},{"cell_type":"code","source":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:52.126138Z","iopub.execute_input":"2024-12-23T04:46:52.126504Z","iopub.status.idle":"2024-12-23T04:46:52.131179Z","shell.execute_reply.started":"2024-12-23T04:46:52.126474Z","shell.execute_reply":"2024-12-23T04:46:52.130322Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"### Creating n-grams (such as bigrams and trigrams) to improve the textual representation and expand word interactions\n","metadata":{}},{"cell_type":"code","source":"# إنشاء n-grams (مثل bigrams و trigrams) لتحسين التمثيل النصي وتوسيع التفاعلات بين الكلمات\ndef make_n_grams(texts):\n    bigram = gensim.models.Phrases(texts, min_count=5, threshold=100)\n    bigram_mod = gensim.models.phrases.Phraser(bigram)\n    trigram = gensim.models.Phrases(bigram[texts], threshold=100)\n    trigram_mod = gensim.models.phrases.Phraser(trigram)\n    bigrams_text = [bigram_mod[doc] for doc in texts]\n    trigrams_text = [trigram_mod[bigram_mod[doc]] for doc in bigrams_text]\n    return trigrams_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:52.132308Z","iopub.execute_input":"2024-12-23T04:46:52.132979Z","iopub.status.idle":"2024-12-23T04:46:52.142997Z","shell.execute_reply.started":"2024-12-23T04:46:52.132924Z","shell.execute_reply":"2024-12-23T04:46:52.142374Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# Removing stopwords from the texts to improve the quality of analysis\n","metadata":{}},{"cell_type":"code","source":"# إزالة الكلمات الشائعة (stopwords) من النصوص لتحسين جودة التحليل\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in gensim.parsing.preprocessing.STOPWORDS] for doc in texts]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:52.143898Z","iopub.execute_input":"2024-12-23T04:46:52.144233Z","iopub.status.idle":"2024-12-23T04:46:52.152830Z","shell.execute_reply.started":"2024-12-23T04:46:52.144195Z","shell.execute_reply":"2024-12-23T04:46:52.152217Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"### Lemmatizing the words using spacy to better analyze the words\n","metadata":{}},{"cell_type":"code","source":"# تحويل الكلمات إلى جذورها باستخدام مكتبة spacy لتحليل الكلمات بشكل أفضل\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent))\n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:52.153696Z","iopub.execute_input":"2024-12-23T04:46:52.154014Z","iopub.status.idle":"2024-12-23T04:46:52.160680Z","shell.execute_reply.started":"2024-12-23T04:46:52.153969Z","shell.execute_reply":"2024-12-23T04:46:52.159924Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Extracting the top words from each cluster using the word distribution and comparing them\n","metadata":{}},{"cell_type":"code","source":"# استخراج أفضل الكلمات من كل مجموعة (cluster) باستخدام توزيع الكلمات والمقارنة بينها\ndef top_words(mgp, cluster_word_distribution, top_cluster, values):\n    Text = ''\n    TheseResults = []\n    for cluster in top_cluster:\n        sort_dicts = sorted(mgp.cluster_word_distribution[cluster].items(), key=lambda k: k[1], reverse=True)[:values]\n        Text += \"\\nCluster %s : %s\" % (cluster, sort_dicts)\n        TheseResults.append([cluster, sort_dicts])\n    return Text, TheseResults\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:52.161539Z","iopub.execute_input":"2024-12-23T04:46:52.161851Z","iopub.status.idle":"2024-12-23T04:46:52.170375Z","shell.execute_reply.started":"2024-12-23T04:46:52.161827Z","shell.execute_reply":"2024-12-23T04:46:52.169612Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"### Preparing the data for training by tokenizing tweets, creating n-grams, lemmatizing words, and removing stopwords\n","metadata":{}},{"cell_type":"code","source":"# تجهيز البيانات للتدريب عن طريق تحويل التغريدات إلى كلمات، وإنشاء n-grams، وتحويل الكلمات إلى جذورها، وإزالة الكلمات الشائعة\ntokens_reviews = list(sent_to_words(data['OriginalTweet']))\ntokens_reviews = make_n_grams(tokens_reviews)\nreviews_lemmatized = lemmatization(tokens_reviews, allowed_postags=['NOUN', 'VERB', 'ADV'])\nreviews_lemmatized = remove_stopwords(reviews_lemmatized)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:46:52.172703Z","iopub.execute_input":"2024-12-23T04:46:52.172976Z","iopub.status.idle":"2024-12-23T04:49:21.324023Z","shell.execute_reply.started":"2024-12-23T04:46:52.172940Z","shell.execute_reply":"2024-12-23T04:49:21.323320Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Training using different values for Alpha and Beta to select the best combination based on performance\n","metadata":{}},{"cell_type":"code","source":"# التدريب باستخدام قيم مختلفة لـ Alpha و Beta لاختيار الأفضل بناءً على الأداء\nResults = []\nX = 0\nK = 4  # عدد المجموعات (clusters)\nfor Alpha in list(np.linspace(0.05, 1, 5)):  # استخدام قيم مختلفة لـ Alpha\n    for Beta in list(np.linspace(0.05, 1, 5)):  # استخدام قيم مختلفة لـ Beta\n        X += 1\n        print(f'Phase Number {X}')\n        mgp = MovieGroupProcess(K=K, alpha=Alpha, beta=Beta, n_iters=5)\n        vocab = set(x for review in reviews_lemmatized for x in review)\n        n_terms = len(vocab)\n        model = mgp.fit(reviews_lemmatized, n_terms)\n        doc_count = np.array(mgp.cluster_doc_count)\n        top_index = doc_count.argsort()[-10:][::-1]\n        Loss = 0\n        for i in range(K):\n            Loss += abs((1 / K) - (doc_count[i] / sum(doc_count)))\n        Results.append({'Parameters': [Alpha, Beta],\n                        'Loss': Loss,\n                        'Doc Number': doc_count,\n                        'Top Index': doc_count.argsort()[-10:][::-1],\n                        'Top Words': top_words(mgp, mgp.cluster_word_distribution, top_index, 10)[0],\n                        'All Words': top_words(mgp, mgp.cluster_word_distribution, top_index, 100)[1]})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T04:49:21.324971Z","iopub.execute_input":"2024-12-23T04:49:21.325219Z","iopub.status.idle":"2024-12-23T05:00:35.336214Z","shell.execute_reply.started":"2024-12-23T04:49:21.325194Z","shell.execute_reply":"2024-12-23T05:00:35.335220Z"}},"outputs":[{"name":"stdout","text":"Phase Number 1\nIn stage 0: transferred 27414 clusters with 4 clusters populated\nIn stage 1: transferred 20230 clusters with 4 clusters populated\nIn stage 2: transferred 14793 clusters with 4 clusters populated\nIn stage 3: transferred 9974 clusters with 4 clusters populated\nIn stage 4: transferred 7928 clusters with 4 clusters populated\nPhase Number 2\nIn stage 0: transferred 28494 clusters with 4 clusters populated\nIn stage 1: transferred 22228 clusters with 4 clusters populated\nIn stage 2: transferred 12371 clusters with 4 clusters populated\nIn stage 3: transferred 7732 clusters with 4 clusters populated\nIn stage 4: transferred 6502 clusters with 4 clusters populated\nPhase Number 3\nIn stage 0: transferred 28851 clusters with 4 clusters populated\nIn stage 1: transferred 23278 clusters with 4 clusters populated\nIn stage 2: transferred 13377 clusters with 4 clusters populated\nIn stage 3: transferred 7774 clusters with 4 clusters populated\nIn stage 4: transferred 6398 clusters with 4 clusters populated\nPhase Number 4\nIn stage 0: transferred 29299 clusters with 4 clusters populated\nIn stage 1: transferred 22205 clusters with 4 clusters populated\nIn stage 2: transferred 11523 clusters with 4 clusters populated\nIn stage 3: transferred 7746 clusters with 4 clusters populated\nIn stage 4: transferred 6367 clusters with 4 clusters populated\nPhase Number 5\nIn stage 0: transferred 29510 clusters with 4 clusters populated\nIn stage 1: transferred 23143 clusters with 4 clusters populated\nIn stage 2: transferred 11795 clusters with 4 clusters populated\nIn stage 3: transferred 6774 clusters with 4 clusters populated\nIn stage 4: transferred 5534 clusters with 4 clusters populated\nPhase Number 6\nIn stage 0: transferred 27599 clusters with 4 clusters populated\nIn stage 1: transferred 20362 clusters with 4 clusters populated\nIn stage 2: transferred 13930 clusters with 4 clusters populated\nIn stage 3: transferred 9050 clusters with 4 clusters populated\nIn stage 4: transferred 6922 clusters with 4 clusters populated\nPhase Number 7\nIn stage 0: transferred 28422 clusters with 4 clusters populated\nIn stage 1: transferred 22044 clusters with 4 clusters populated\nIn stage 2: transferred 13112 clusters with 4 clusters populated\nIn stage 3: transferred 8581 clusters with 4 clusters populated\nIn stage 4: transferred 6668 clusters with 4 clusters populated\nPhase Number 8\nIn stage 0: transferred 28769 clusters with 4 clusters populated\nIn stage 1: transferred 23204 clusters with 4 clusters populated\nIn stage 2: transferred 13690 clusters with 4 clusters populated\nIn stage 3: transferred 8414 clusters with 4 clusters populated\nIn stage 4: transferred 6863 clusters with 4 clusters populated\nPhase Number 9\nIn stage 0: transferred 29063 clusters with 4 clusters populated\nIn stage 1: transferred 22525 clusters with 4 clusters populated\nIn stage 2: transferred 13573 clusters with 4 clusters populated\nIn stage 3: transferred 7615 clusters with 4 clusters populated\nIn stage 4: transferred 4476 clusters with 4 clusters populated\nPhase Number 10\nIn stage 0: transferred 29377 clusters with 4 clusters populated\nIn stage 1: transferred 23092 clusters with 4 clusters populated\nIn stage 2: transferred 9116 clusters with 4 clusters populated\nIn stage 3: transferred 4355 clusters with 4 clusters populated\nIn stage 4: transferred 3398 clusters with 4 clusters populated\nPhase Number 11\nIn stage 0: transferred 27636 clusters with 4 clusters populated\nIn stage 1: transferred 20268 clusters with 4 clusters populated\nIn stage 2: transferred 13237 clusters with 4 clusters populated\nIn stage 3: transferred 9150 clusters with 4 clusters populated\nIn stage 4: transferred 7420 clusters with 4 clusters populated\nPhase Number 12\nIn stage 0: transferred 28682 clusters with 4 clusters populated\nIn stage 1: transferred 21028 clusters with 4 clusters populated\nIn stage 2: transferred 11285 clusters with 4 clusters populated\nIn stage 3: transferred 7567 clusters with 4 clusters populated\nIn stage 4: transferred 6102 clusters with 4 clusters populated\nPhase Number 13\nIn stage 0: transferred 28924 clusters with 4 clusters populated\nIn stage 1: transferred 23606 clusters with 4 clusters populated\nIn stage 2: transferred 13594 clusters with 4 clusters populated\nIn stage 3: transferred 7256 clusters with 4 clusters populated\nIn stage 4: transferred 5234 clusters with 4 clusters populated\nPhase Number 14\nIn stage 0: transferred 29114 clusters with 4 clusters populated\nIn stage 1: transferred 23434 clusters with 4 clusters populated\nIn stage 2: transferred 15196 clusters with 4 clusters populated\nIn stage 3: transferred 10792 clusters with 4 clusters populated\nIn stage 4: transferred 8012 clusters with 4 clusters populated\nPhase Number 15\nIn stage 0: transferred 29433 clusters with 4 clusters populated\nIn stage 1: transferred 24177 clusters with 4 clusters populated\nIn stage 2: transferred 12501 clusters with 4 clusters populated\nIn stage 3: transferred 5712 clusters with 4 clusters populated\nIn stage 4: transferred 3921 clusters with 4 clusters populated\nPhase Number 16\nIn stage 0: transferred 27423 clusters with 4 clusters populated\nIn stage 1: transferred 20378 clusters with 4 clusters populated\nIn stage 2: transferred 15223 clusters with 4 clusters populated\nIn stage 3: transferred 9782 clusters with 4 clusters populated\nIn stage 4: transferred 7261 clusters with 4 clusters populated\nPhase Number 17\nIn stage 0: transferred 28400 clusters with 4 clusters populated\nIn stage 1: transferred 22187 clusters with 4 clusters populated\nIn stage 2: transferred 12764 clusters with 4 clusters populated\nIn stage 3: transferred 7156 clusters with 4 clusters populated\nIn stage 4: transferred 5429 clusters with 4 clusters populated\nPhase Number 18\nIn stage 0: transferred 28754 clusters with 4 clusters populated\nIn stage 1: transferred 22962 clusters with 4 clusters populated\nIn stage 2: transferred 16506 clusters with 4 clusters populated\nIn stage 3: transferred 10223 clusters with 4 clusters populated\nIn stage 4: transferred 8025 clusters with 4 clusters populated\nPhase Number 19\nIn stage 0: transferred 29122 clusters with 4 clusters populated\nIn stage 1: transferred 23336 clusters with 4 clusters populated\nIn stage 2: transferred 12775 clusters with 4 clusters populated\nIn stage 3: transferred 6751 clusters with 4 clusters populated\nIn stage 4: transferred 4978 clusters with 4 clusters populated\nPhase Number 20\nIn stage 0: transferred 29338 clusters with 4 clusters populated\nIn stage 1: transferred 23949 clusters with 4 clusters populated\nIn stage 2: transferred 10994 clusters with 4 clusters populated\nIn stage 3: transferred 4640 clusters with 4 clusters populated\nIn stage 4: transferred 3115 clusters with 3 clusters populated\nPhase Number 21\nIn stage 0: transferred 27460 clusters with 4 clusters populated\nIn stage 1: transferred 20058 clusters with 4 clusters populated\nIn stage 2: transferred 13341 clusters with 4 clusters populated\nIn stage 3: transferred 8890 clusters with 4 clusters populated\nIn stage 4: transferred 6788 clusters with 4 clusters populated\nPhase Number 22\nIn stage 0: transferred 28515 clusters with 4 clusters populated\nIn stage 1: transferred 22466 clusters with 4 clusters populated\nIn stage 2: transferred 13713 clusters with 4 clusters populated\nIn stage 3: transferred 8546 clusters with 4 clusters populated\nIn stage 4: transferred 6499 clusters with 4 clusters populated\nPhase Number 23\nIn stage 0: transferred 29018 clusters with 4 clusters populated\nIn stage 1: transferred 22948 clusters with 4 clusters populated\nIn stage 2: transferred 13197 clusters with 4 clusters populated\nIn stage 3: transferred 7895 clusters with 4 clusters populated\nIn stage 4: transferred 6283 clusters with 4 clusters populated\nPhase Number 24\nIn stage 0: transferred 29216 clusters with 4 clusters populated\nIn stage 1: transferred 23061 clusters with 4 clusters populated\nIn stage 2: transferred 12235 clusters with 4 clusters populated\nIn stage 3: transferred 7343 clusters with 4 clusters populated\nIn stage 4: transferred 5653 clusters with 4 clusters populated\nPhase Number 25\nIn stage 0: transferred 29338 clusters with 4 clusters populated\nIn stage 1: transferred 24502 clusters with 4 clusters populated\nIn stage 2: transferred 14495 clusters with 4 clusters populated\nIn stage 3: transferred 7531 clusters with 4 clusters populated\nIn stage 4: transferred 5086 clusters with 4 clusters populated\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# تحويل النتائج إلى DataFrame\nResultsDF = pd.DataFrame(columns=['Parameters', 'Loss', 'Doc Number', 'Top Index', 'Top Words', 'All Words'])\nfor n, i in enumerate(Results):\n    ResultsDF.loc[n] = list(i.values())\n\n# استخراج أفضل قيم لـ Alpha و Beta\nbest_result = ResultsDF.loc[ResultsDF['Loss'].idxmin()]\nbest_alpha, best_beta = best_result['Parameters']\nprint(f\"أفضل قيمة لـ Alpha: {best_alpha}, وأفضل قيمة لـ Beta: {best_beta}\")\n\n# استخدام أفضل القيم للتدريب النهائي\nmgp = MovieGroupProcess(K=K, alpha=best_alpha, beta=best_beta, n_iters=5)\nvocab = set(x for review in reviews_lemmatized for x in review)\nn_terms = len(vocab)\nmodel = mgp.fit(reviews_lemmatized, n_terms)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:00:35.337403Z","iopub.execute_input":"2024-12-23T05:00:35.337681Z","iopub.status.idle":"2024-12-23T05:01:01.352332Z","shell.execute_reply.started":"2024-12-23T05:00:35.337654Z","shell.execute_reply":"2024-12-23T05:01:01.351428Z"}},"outputs":[{"name":"stdout","text":"أفضل قيمة لـ Alpha: 0.7625, وأفضل قيمة لـ Beta: 0.05\nIn stage 0: transferred 27475 clusters with 4 clusters populated\nIn stage 1: transferred 20365 clusters with 4 clusters populated\nIn stage 2: transferred 13419 clusters with 4 clusters populated\nIn stage 3: transferred 8904 clusters with 4 clusters populated\nIn stage 4: transferred 6814 clusters with 4 clusters populated\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# تحديد الكلاستر لكل تغريدة\ndef choose_best_label(review, mgp):\n    scores = []\n    for cluster in range(K):\n        score = 0\n        for word in review:\n            if word in mgp.cluster_word_distribution[cluster]:\n                score += np.log(mgp.cluster_word_distribution[cluster][word] + 1) - np.log(mgp.cluster_word_count[cluster] + len(mgp.cluster_word_distribution[cluster]))\n        scores.append(score)\n    return np.argmax(scores)\n\n# إضافة عمود \"Cluster\" يحتوي على الكلاستر لكل تغريدة\ndata['Cluster'] = [choose_best_label(review, mgp) for review in reviews_lemmatized]\n\n# حفظ النتائج في ملف CSV\noutput_file = 'ClusteredTweets.csv'\ndata.to_csv(output_file, index=False)\n\n# طباعة الخرج النهائي\nprint(\"تم تصنيف التويترات وحفظ النتائج في ملف ClusteredTweets.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-23T05:01:01.353372Z","iopub.execute_input":"2024-12-23T05:01:01.353713Z","iopub.status.idle":"2024-12-23T05:01:07.519994Z","shell.execute_reply.started":"2024-12-23T05:01:01.353679Z","shell.execute_reply":"2024-12-23T05:01:07.519126Z"}},"outputs":[{"name":"stdout","text":"تم تصنيف التويترات وحفظ النتائج في ملف ClusteredTweets.csv\n","output_type":"stream"}],"execution_count":24}]}